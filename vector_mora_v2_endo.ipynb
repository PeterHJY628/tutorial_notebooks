{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/vector_mora_v2_endo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBetGEmbb0rO",
        "outputId": "429b6419-0b34-43a3-ef3b-337d99b3bf89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/PitVQA/datasets’: No such file or directory\n",
            "[Errno 2] No such file or directory: '/content/PitVQA/datasets'\n",
            "/content\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
            "From (redirected): https://drive.google.com/uc?id=1FoAEY_u0PTAlrscjEifi2om15A83wL78&confirm=t&uuid=e6d90e00-1461-49c2-8e37-8ecbea0786f5\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.71G/2.71G [00:54<00:00, 49.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Download Dataset\n",
        "!mkdir /content/PitVQA/datasets\n",
        "%cd /content/PitVQA/datasets\n",
        "!gdown --id 1FoAEY_u0PTAlrscjEifi2om15A83wL78\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm==0.9.12 fairscale==0.4.13 scikit-learn==1.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIu-x847cUHk",
        "outputId": "1d68382d-3c08-4e89-854d-5defd64cb537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from pathlib import Path\n",
        "\n",
        "# model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import ViTModel, BlipConfig, BlipTextModel\n",
        "\n",
        "# main\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "qP9byxmaclX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "mqZaGdyscp2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EndoVis18VQAGen(Dataset):\n",
        "    def __init__(self, seq, folder_head, folder_tail):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq:\n",
        "            filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' % (len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qa_full_path = Path(self.vqas[idx][0])\n",
        "        seq_path = qa_full_path.parents[2]\n",
        "        file_name = self.vqas[idx][0].split('/')[-1]  # / in linux and \\\\ in windows\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(seq_path, 'left_fr', file_name.split('_')[0] + '.png')\n",
        "        raw_image = Image.open(img_loc).convert('RGB')\n",
        "        img = self.transform(raw_image)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "\n",
        "        return img, question, answer"
      ],
      "metadata": {
        "id": "-B1H_ah0cpMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "YS2WopWidNM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))"
      ],
      "metadata": {
        "id": "Ufyak10oc3LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "bsRsRN3ndwWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vector_MoRA(nn.Module):\n",
        "    def __init__(self, w_qkv, mora_rank, lora_dropout):\n",
        "        super().__init__()\n",
        "        self.base_layer = w_qkv  # original c_atten layer\n",
        "        self.r = mora_rank  # one of mora rank elements in the list\n",
        "        self.in_features = w_qkv.weight.shape[0]  # 768\n",
        "        self.out_features = w_qkv.weight.shape[1]  # 2304\n",
        "\n",
        "        # LoRA dropout\n",
        "        self.lora_dropout = nn.ModuleDict({\n",
        "            'default': nn.Dropout(p=lora_dropout)\n",
        "        })\n",
        "\n",
        "        # LoRA A and B matrices\n",
        "        self.lora_A = nn.ModuleDict({\n",
        "            'default': nn.Conv1d(self.r, self.r, bias=False, kernel_size=1)\n",
        "        })\n",
        "        # self.lora_A = nn.ModuleDict({\n",
        "        #     'default': nn.Sequential(\n",
        "        #         nn.Conv1d(self.r, self.r, bias=False, kernel_size=1),\n",
        "        #         nn.ReLU(),\n",
        "        #         nn.Linear(self.r, self.r, bias=False)\n",
        "        #     )\n",
        "        # })\n",
        "        nn.init.zeros_(self.lora_A['default'].weight)\n",
        "        self.lora_B = self.lora_A  # not for use\n",
        "\n",
        "        # For Embedding layer\n",
        "        self.lora_embedding_A = nn.ParameterDict({})\n",
        "        self.lora_embedding_B = nn.ParameterDict({})\n",
        "\n",
        "    def forward(self, x):  # [32, 32, 768]\n",
        "        # Original output\n",
        "        result = self.base_layer(x)  # [32, 32, 2304]\n",
        "        x = self.lora_dropout['default'](x)  # x is the input for mora\n",
        "\n",
        "        '''apply compression before lora_A'''  # RoPE\n",
        "        in_f, out_f = self.in_features, self.out_features\n",
        "        r = self.r\n",
        "        # suppose mora_type = 6\n",
        "        sum_inter = in_f // r\n",
        "        rb1 = in_f // r if in_f % r == 0 else in_f // r + 1\n",
        "\n",
        "        if in_f % r != 0:\n",
        "            pad_size = r - in_f % r\n",
        "            x = torch.cat([x, x[..., :pad_size]], dim=-1)  # [32, 32, 960]\n",
        "            sum_inter += 1\n",
        "        in_x = x.view(*x.shape[:-1], sum_inter, r)  # [32, 32, 3, 256]\n",
        "\n",
        "        if not hasattr(self, 'cos') and not hasattr(self, 'sin'):\n",
        "            inv_freq = 1.0 / (10000 ** (torch.arange(0, r, 2).float() / r))\n",
        "            t = torch.arange(rb1)\n",
        "            freqs = torch.outer(t, inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1)\n",
        "            self.cos = emb.cos().unsqueeze(0).to(x.device).to(x.dtype)\n",
        "            self.sin = emb.sin().unsqueeze(0).to(x.device).to(x.dtype)\n",
        "\n",
        "        rh_in_x = torch.cat((-in_x[..., r // 2:], in_x[..., :r // 2]), dim=-1)\n",
        "        # rh_in_x 最后一个维度的前 r//2 个元素是 in_x 后半部分的负值, rh_in_x 最后一个维度的后 r//2 个元素是 in_x 前半部分的原值\n",
        "        in_x = in_x * self.cos + rh_in_x * self.sin  # [32, 32, 3, 256]\n",
        "\n",
        "        # rearrange features\n",
        "        b, c, h, w = in_x.shape  # [32, 32, 3, 256]\n",
        "        in_x = in_x.view(b, c*h, w).permute(0, 2, 1)  # [32, 256, 96]\n",
        "\n",
        "        '''apply lora_A'''\n",
        "        out_x = self.lora_A['default'](in_x)  # [32, 256, 96]\n",
        "        out_x = out_x.view(b, c, h, w)  # [32, 32, 3, 256]\n",
        "\n",
        "        '''apply decompression after lora_A'''\n",
        "        # suppose mora_type = 6\n",
        "        out_x = out_x.view(*x.shape[:-1], -1)[..., :out_f]  # [32, 32, 768]\n",
        "        if out_x.shape[-1] < out_f:\n",
        "            repeat_time = out_f // out_x.shape[-1]\n",
        "            if out_f % out_x.shape[-1] != 0:\n",
        "                repeat_time += 1\n",
        "            out_x = torch.cat([out_x] * repeat_time, dim=-1)[..., :out_f]  # [32, 32, 2304]\n",
        "\n",
        "        return result + out_x\n",
        "\n",
        "class VectorMoRAInitializer:\n",
        "    def __init__(self, model, base_rank=8, mora_rank_coefficients=None, lora_dropout=0.01):\n",
        "        self.model = model\n",
        "        self.base_rank = base_rank\n",
        "        self.lora_dropout = lora_dropout\n",
        "        if mora_rank_coefficients is None:\n",
        "            self.mora_rank_coefficients = [32, 32, 30, 30, 28, 28, 26, 26, 24, 24, 22, 22]\n",
        "        else:\n",
        "            self.mora_rank_coefficients = mora_rank_coefficients\n",
        "\n",
        "    def calculate_mora_ranks(self):\n",
        "        return [self.base_rank * coeff for coeff in self.mora_rank_coefficients]\n",
        "\n",
        "    def initialize_mora(self):\n",
        "        mora_ranks = self.calculate_mora_ranks()\n",
        "\n",
        "        for param in self.model.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for t_layer_i, blk in enumerate(self.model.transformer.h):\n",
        "            w_qkv = blk.attn.c_attn\n",
        "            mora_rank = mora_ranks[t_layer_i]\n",
        "            blk.attn.c_attn = Vector_MoRA(w_qkv, mora_rank, self.lora_dropout) # add vec_mora to blk.attn.c_attn\n",
        "\n",
        "        print(\"Vector MoRA params initialized!\")\n",
        "        return self.model\n",
        "\n",
        "class PitVQAGen(nn.Module):\n",
        "    def __init__(self, base_rank=8, mora_rank_coefficients=None):\n",
        "        super(PitVQAGen, self).__init__()\n",
        "\n",
        "        # visual encoder\n",
        "        model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "        self.visual_encoder = ViTModel.from_pretrained(model_name)\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # end of string\n",
        "\n",
        "        # text encoder\n",
        "        # config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "        # self.text_encoder = BlipTextModel(config.text_config, add_pooling_layer=False)\n",
        "        self.text_encoder = BlipTextModel.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "\n",
        "        # modify embedding layer\n",
        "        new_vocab_size = len(self.tokenizer)\n",
        "        embedding_dim = self.text_encoder.embeddings.word_embeddings.embedding_dim\n",
        "        self.text_encoder.embeddings.word_embeddings = nn.Embedding(new_vocab_size, embedding_dim)  # He init\n",
        "\n",
        "        # gpt2 decoder\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        if mora_rank_coefficients is None:\n",
        "            mora_rank_coefficients = [32, 32, 30, 30, 28, 28, 26, 26, 24, 24, 22, 22]\n",
        "            self.gpt = VectorMoRAInitializer(self.gpt, base_rank=base_rank,\n",
        "                                             mora_rank_coefficients=mora_rank_coefficients).initialize_mora()\n",
        "        else:\n",
        "            self.gpt = VectorMoRAInitializer(self.gpt, base_rank=base_rank,\n",
        "                                             mora_rank_coefficients=mora_rank_coefficients).initialize_mora()\n",
        "\n",
        "    def forward(self, image, question_inputs):\n",
        "        # visual encoder\n",
        "        image = image.to(device)\n",
        "        image_embeds = self.visual_encoder(image).last_hidden_state  # torch.Size([bs, 197, 768])\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)  # torch.Size([bs, 197])\n",
        "\n",
        "        question_input_ids = question_inputs['input_ids']  # torch.Size([bs, 25])\n",
        "        question_att_mask = question_inputs['attention_mask']\n",
        "        print('question_input_ids:', question_input_ids.shape)\n",
        "        print('image_embeds:', image_embeds.shape)\n",
        "        # multimodal encoder\n",
        "        text_output = self.text_encoder(input_ids=question_input_ids,\n",
        "                                        attention_mask=question_att_mask,\n",
        "                                        encoder_hidden_states=image_embeds,\n",
        "                                        encoder_attention_mask=image_atts,\n",
        "                                        return_dict=True)\n",
        "        text_embeds = text_output.last_hidden_state  # torch.Size([bs, 25, 768]), args.question_len=25\n",
        "        print('text_embeds', text_embeds.shape)\n",
        "\n",
        "        # text decoder\n",
        "        gpt_output = self.gpt(inputs_embeds=text_embeds,\n",
        "                              encoder_attention_mask=question_att_mask)  # torch.Size([bs, 25, 50257])\n",
        "        return gpt_output.logits"
      ],
      "metadata": {
        "id": "UYoYGYomdjj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "AhYY9n37mtj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "    model.train()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, (images, questions, answers) in tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader), desc=\"Training\"):\n",
        "        question_inputs = tokenizer(questions, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                    return_tensors=\"pt\", truncation=True)\n",
        "        answer_inputs = tokenizer(answers, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                  return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "        # get logits and labels\n",
        "        logits = model(image=images.to(device), question_inputs=question_inputs.to(device))\n",
        "        # 文本生成模型的输出代表模型对每个位置的下一个词的预测\n",
        "        labels = answer_inputs['input_ids'].to(device)\n",
        "\n",
        "        # get shifted logits and labels\n",
        "        shift_logits = logits[:, :-1, :].contiguous()\n",
        "        shift_labels = labels[:, 1:].contiguous()\n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "    print(\"\\033[K\\nEpoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, questions, answers) in tqdm(enumerate(val_loader, 0), total=len(val_loader), desc=\"Validating\"):\n",
        "            question_inputs = tokenizer(questions, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                        return_tensors=\"pt\", truncation=True)\n",
        "            answer_inputs = tokenizer(answers, padding=\"max_length\", max_length=int(args.seq_length),\n",
        "                                      return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "            # get logits and labels\n",
        "            logits = model(image=images.to(device), question_inputs=question_inputs.to(device))\n",
        "            labels = answer_inputs['input_ids'].to(device)\n",
        "\n",
        "            # get shifted logits and labels\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # generate predicted answer\n",
        "            _, predicted = torch.max(logits, dim=-1)\n",
        "\n",
        "            # decode references and predictions\n",
        "            reference_answers = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predicted_answers = tokenizer.batch_decode(predicted, skip_special_tokens=True)\n",
        "\n",
        "            # add references and hypotheses to lists\n",
        "            for ref, hyp in zip(reference_answers, predicted_answers):\n",
        "                references.append([ref.split()])\n",
        "                hypotheses.append(hyp.split())\n",
        "\n",
        "        # Calculate BLEU_1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        print(f\"\\033[K\\nEpoch: {epoch}/{args.epochs} EVA LOSS: {total_loss.avg:.6f} \"\n",
        "              f\"BLEU-1: {metrics['Bleu_1']:.6f} BLEU-2: {metrics['Bleu_2']:.6f} \"\n",
        "              f\"BLEU-3: {metrics['Bleu_3']:.6f} BLEU-4: {metrics['Bleu_4']:.6f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "class Hyperparameters:\n",
        "    epochs = 50\n",
        "    batch_size = 32\n",
        "    workers = 2\n",
        "    random_seed = 42\n",
        "\n",
        "    seq_length = 32\n",
        "    base_rank = 8\n",
        "    coeff = 'vec1'\n",
        "    lr = 0.00002\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # hyperparameter\n",
        "    args = Hyperparameters()\n",
        "\n",
        "    seed_everything(args.random_seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = '/content/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    # dataloader\n",
        "    train_dataset = EndoVis18VQAGen(train_seq, folder_head, folder_tail)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
        "                    shuffle=True, num_workers=args.workers)\n",
        "    val_dataset = EndoVis18VQAGen(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
        "                    shuffle=False, num_workers=args.workers)\n",
        "\n",
        "    # init tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    vector = []\n",
        "    if args.coeff == 'vec1':\n",
        "        vector = [32, 32, 30, 30, 28, 28, 26, 26, 24, 24, 22, 22]\n",
        "    elif args.coeff == 'vec2':\n",
        "        vector = [64, 64, 60, 60, 56, 56, 52, 52, 48, 48, 44, 44]\n",
        "    elif args.coeff == 'vec3':\n",
        "        vector = [24, 24, 22, 22, 20, 20, 18, 18, 16, 16, 14, 14]\n",
        "    model = PitVQAGen(base_rank=args.base_rank, mora_rank_coefficients=vector)\n",
        "    model = model.to(device)\n",
        "\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    # init optimizer and criterion\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # train and validation\n",
        "    print('Start training.')\n",
        "    for epoch in range(start_epoch, args.epochs+1):\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model=model, criterion=criterion, optimizer=optimizer,\n",
        "              epoch=epoch, tokenizer=tokenizer, device=device)\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model=model, criterion=criterion, epoch=epoch,\n",
        "                           tokenizer=tokenizer, device=device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            print(f'Best epoch: {epoch}, Best Bleu_4: {metrics[\"Bleu_4\"]}')\n",
        "            # save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement,\n",
        "            #                     model, optimizer, best_results[0], final_args=None)\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    print('End training.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "BSnGpYN2mtBI",
        "outputId": "16235e6e-6f94-4e79-ec07-ff916e43b524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BlipTextModel were not initialized from the model checkpoint at Salesforce/blip-vqa-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector MoRA params initialized!\n",
            "model params:  364401920\n",
            "Start training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/331 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question_input_ids: torch.Size([32, 32])\n",
            "image_embeds: torch.Size([32, 197, 768])\n",
            "text_embeds torch.Size([32, 32, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/331 [00:07<39:25,  7.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question_input_ids: torch.Size([32, 32])\n",
            "image_embeds: torch.Size([32, 197, 768])\n",
            "text_embeds torch.Size([32, 32, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 2/331 [00:08<21:10,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question_input_ids: torch.Size([32, 32])\n",
            "image_embeds: torch.Size([32, 197, 768])\n",
            "text_embeds torch.Size([32, 32, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/331 [00:10<15:19,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question_input_ids: torch.Size([32, 32])\n",
            "image_embeds: torch.Size([32, 197, 768])\n",
            "text_embeds torch.Size([32, 32, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/331 [00:11<21:32,  3.94s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f6fddf0cd30c>\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         train(args, train_dataloader=train_dataloader, model=model, criterion=criterion, optimizer=optimizer,\n\u001b[0m\u001b[1;32m    159\u001b[0m               epoch=epoch, tokenizer=tokenizer, device=device)\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-f6fddf0cd30c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[K\\nEpoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csYcOih6L7Dz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}